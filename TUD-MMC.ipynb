{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Results of \"TUD-MMC at MediaEval 2016: Context of Experience task\" by Wang & Liem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "train_path = \"res/coe_dataset_icpr/dev_set/\"\n",
    "test_path = \"res/coe_dataset_icpr/test_set/\"\n",
    "\n",
    "audio_folder = \"audio_descriptors/\"\n",
    "text_folder = \"text_descriptors/\"\n",
    "vis_folder = \"vis_descriptors/\"\n",
    "metadata_folder = \"XML/\"\n",
    "\n",
    "train_entries_path = \"res/CoeTraining.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Features are built in the manner described in the paper of Wang & Liem or \"Right Inflight? A Dataset for Exploring the Automatic\n",
    "Prediction of Movies Suitable for a Watching Situation\" (https://mmsys2016.itec.aau.at/papers/MMSYS/a45-riegler.pdf), if Wang & Liem do not provide any information.\n",
    "\n",
    "This leads to following set-up:\n",
    "\n",
    "Metadata: (language, year published, genre, country, runtime and age rating) - from XML -- 1-Hot Encoding for all categorical values<br>\n",
    "Text: as is td-idf <br>\n",
    "Audio: Averaged of all Frames (NaN to 0) - Mel-Frequency Cepstral Coefficients<br>\n",
    "Visual: as is - Histogram of Oriented Gradients (HOG) gray, Color Moments, local binary patterns (LBP) and Gray Level Run Length Matrix\n",
    "\n",
    "NOTE: Training data - invalid entry (2_states, also in test set), (Moulin_Rouge!.mp4, should be Moulin_Rouge! --> fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>file_name</th>\n",
       "      <th>goodforairplanes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Fish Called Wanda</td>\n",
       "      <td>A_Fish_Called_Wanda</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Goofy Movie</td>\n",
       "      <td>A_Goofy_Movie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Million Ways to Die in the West</td>\n",
       "      <td>A_Million_Ways_to_Die_in_the_West</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Single Man</td>\n",
       "      <td>A_Single_Man</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Gangster</td>\n",
       "      <td>American_Gangster</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          movie_name                          file_name  \\\n",
       "0                A Fish Called Wanda                A_Fish_Called_Wanda   \n",
       "1                      A Goofy Movie                      A_Goofy_Movie   \n",
       "2  A Million Ways to Die in the West  A_Million_Ways_to_Die_in_the_West   \n",
       "3                       A Single Man                       A_Single_Man   \n",
       "4                  American Gangster                  American_Gangster   \n",
       "\n",
       "  goodforairplanes  \n",
       "0                1  \n",
       "1                0  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_train = pd.read_csv(train_entries_path)\n",
    "df_base_train = df_base_train[df_base_train['file_name'] != '2_States'] # remove invalid entry\n",
    "df_base_train.sort_values(by='file_name', inplace=True)\n",
    "df_base_train.reset_index(inplace=True, drop=True)\n",
    "df_targets_train = df_base_train['goodforairplanes'].astype(int)\n",
    "df_base_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractors\n",
    "As the dataset was built in a manner that would have been considered dirty already in 2002 a lot of feature extraction is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_features(file_name, use_train=True):\n",
    "    \"\"\"\n",
    "        returns 1x14 dataframe, with averaged Mel-Frequency Cepstral Coefficients + file_name\n",
    "    \"\"\"\n",
    "    base_path = train_path if use_train else test_path\n",
    "    file_path = os.path.join(base_path, audio_folder, file_name + \".csv\")\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(file_name, \" does not exist!\")\n",
    "        return pd.DataFrame(columns=[str(x) for x in range(13)] + ['file_name'])\n",
    "    df_audio = pd.read_csv(file_path, header=None).T # transpose (columns are rows)\n",
    "    df_audio = df_audio.fillna(0) # nan values are treated as 0\n",
    "    df_audio = pd.DataFrame(df_audio.mean(axis=0)).T # average accross columns\n",
    "    df_audio['file_name'] = file_name\n",
    "    return df_audio\n",
    "\n",
    "def get_all_audio_features(df, use_train=True):\n",
    "    \"\"\"\n",
    "        returns nx14 dataframe, containing audio features for all movies\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file_name in df['file_name']:\n",
    "        dfs.append(get_audio_features(file_name, use_train))\n",
    "    \n",
    "    return pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "def get_all_text_features(df, use_train=True):\n",
    "    \"\"\"\n",
    "        returns nx3284 dataframe, containing tf-idf features for all movies\n",
    "        the dataset creators messed up - contains several terms multiple times\n",
    "        ordered alphabetically (?) - Live_Nude_Girls and Transformers__Age_of_Extinction where switched (detected perchance)\n",
    "    \"\"\"\n",
    "    base_path = train_path if use_train else test_path\n",
    "    file_path = os.path.join(base_path, text_folder, \"tdf_idf_dev.csv\")\n",
    "    df_txt = pd.read_csv(file_path)\n",
    "    # the creators of the dataset missed how csv-files work - so we transpose and drop empty rows to get the correct format\n",
    "    cols = df_txt.columns \n",
    "    df_txt = df_txt.T.dropna()\n",
    "    df_txt.columns = cols\n",
    "    df_txt.reset_index(inplace=True, drop=True)\n",
    "    df_txt['file_name'] = sorted(df['file_name']) # we assume the info to be order alphabetically, as we do not have more info\n",
    "    return df_txt.reset_index(drop=True)\n",
    "\n",
    "def get_vis_features(file_name, use_train=True):\n",
    "    \"\"\"\n",
    "        returns 1x1653 dataframe, with unspecified visual features + file_name\n",
    "        we assume that every single value in the csv is one feature\n",
    "        this may be wrong, as there are two rows and no documentation (again)\n",
    "    \"\"\"\n",
    "    base_path = train_path if use_train else test_path\n",
    "    file_path = os.path.join(base_path, vis_folder, file_name + \".csv\")\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(file_name, \" does not exist!\")\n",
    "        return pd.DataFrame(columns=[str(x) for x in range(1652)] + ['file_name'])\n",
    "    df_vis = pd.read_csv(file_path, header=None)\n",
    "    df_vis = pd.DataFrame(pd.concat([df_vis.loc[0,:], df_vis.loc[1,:]])).reset_index(drop=True).T # treat each value as single feature (-> no aggregation)\n",
    "    df_vis['file_name'] = file_name\n",
    "    return df_vis\n",
    "\n",
    "def get_all_vis_features(df, use_train=True):\n",
    "    \"\"\"\n",
    "        returns nx1653 dataframe, containing visual features for all movies\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file_name in df['file_name']:\n",
    "        dfs.append(get_vis_features(file_name, use_train))\n",
    "    \n",
    "    return pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "def get_meta_features(file_name, use_train=True):\n",
    "    \"\"\"\n",
    "        returns 1x7 dataframe, with metadata features + file_name\n",
    "        One Hot Encoding is not applied here, this should happen later\n",
    "    \"\"\"\n",
    "    base_path = train_path if use_train else test_path\n",
    "    file_path = os.path.join(base_path, metadata_folder, file_name + \".xml\")\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(file_name, \" does not exist!\")\n",
    "        return pd.DataFrame(columns=['country', 'genre', 'language', 'rated', 'runtime', 'year', 'file_name'])\n",
    "    etree = ET.parse(file_path)\n",
    "    movie = etree.getroot().find('movie')\n",
    "    mv = {}\n",
    "    mv['language'] = [movie.get('language')]\n",
    "    mv['year'] = [int(movie.get('year'))]\n",
    "    mv['genre'] = [movie.get('genre')]\n",
    "    mv['country'] = [movie.get('country')]\n",
    "    mv['runtime'] = [int(movie.get('runtime')[:-4])]\n",
    "    mv['rated'] = [movie.get('rated')]\n",
    "\n",
    "    df_meta = pd.DataFrame.from_dict(mv)\n",
    "    df_meta['file_name'] = file_name\n",
    "    \n",
    "    return df_meta\n",
    "\n",
    "def get_all_meta_features(df, use_train=True):\n",
    "    \"\"\"\n",
    "        returns nx7 dataframe, containing metadata features for all movies\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for file_name in df['file_name']:\n",
    "        dfs.append(get_meta_features(file_name, use_train))\n",
    "    \n",
    "    df_meta = pd.concat(dfs)\n",
    "    \n",
    "    df_country = df_meta.country.str.replace(' ','').str.get_dummies(sep=',')\n",
    "    df_country.columns = ['country_' + x for x in df_country.columns]\n",
    "\n",
    "    df_genre = df_meta.genre.str.replace(' ','').str.get_dummies(sep=',')\n",
    "    df_genre.columns = ['genre_' + x for x in df_genre.columns]\n",
    "\n",
    "    df_language = df_meta.language.str.replace(' ','').str.get_dummies(sep=',')\n",
    "    df_language.columns = ['language_' + x for x in df_language.columns]\n",
    "\n",
    "    df_rated = df_meta.rated.str.get_dummies(sep=',')\n",
    "    df_rated.columns = ['rated_' + x for x in df_rated.columns]\n",
    "    \n",
    "    return pd.concat([df_country, df_genre, df_language, df_rated, df_meta[['runtime', 'year', 'file_name']]], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_audio_train = get_all_audio_features(df_base_train)\n",
    "df_audio_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txt_train = get_all_text_features(df_base_train)\n",
    "df_txt_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vis_train = get_all_vis_features(df_base_train)\n",
    "df_vis_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_train = get_all_meta_features(df_base_train)\n",
    "df_meta_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_gen(n, forbid):\n",
    "    state = dict()\n",
    "    track = dict()\n",
    "    for (i, o) in enumerate(forbid):\n",
    "        x = track.get(o, o)\n",
    "        t = state.get(n-i-1, n-i-1)\n",
    "        state[x] = t\n",
    "        track[t] = x\n",
    "        state.pop(n-i-1, None)\n",
    "        track.pop(o, None)\n",
    "    del track\n",
    "    for remaining in range(n-len(forbid), 0, -1):\n",
    "        i = random.randrange(remaining)\n",
    "        yield state.get(i, i)\n",
    "        state[i] = state.get(remaining - 1, remaining - 1)\n",
    "        state.pop(remaining - 1, None)\n",
    "\n",
    "def randomSet(S,n=0):\n",
    "    Set = []\n",
    "    if (n==0):\n",
    "        n = len(S)\n",
    "    rand = random.randint(1,n)\n",
    "    return np.random.choice(S, rand ,replace=False)\n",
    "\n",
    "def InconCheck(S,D,T):\n",
    "    incon_value = 0\n",
    "    data = D[S]\n",
    "    data['target'] = T\n",
    "    # create dataframe\n",
    "    data_no = data.loc[data['target']==0]\n",
    "    data_no = data_no.drop('target',axis=1)\n",
    "    data_yes = data.loc[data['target']==1]\n",
    "    data_yes = data_yes.drop('target',axis=1)\n",
    "    no_df = pd.DataFrame(data_no.groupby(data_no.columns.tolist(),as_index=False).size().reset_index(name='target_no'))\n",
    "    yes_df = pd.DataFrame(data_yes.groupby(data_yes.columns.tolist(),as_index=False).size().reset_index(name='target_yes'))\n",
    "\n",
    "    result_df = pd.merge(no_df, yes_df, how='outer', on=S)\n",
    "    result_df = result_df.fillna(0)\n",
    "    result_df['inc'] = result_df['target_no']\n",
    "\n",
    "    for index, row in result_df.iterrows():\n",
    "        if (row['target_yes']>row['target_no']):\n",
    "            result_df.iloc[index,result_df.columns=='inc'] = row['target_no']\n",
    "        else:\n",
    "            result_df.iloc[index,result_df.columns=='inc'] = row['target_yes']\n",
    "    \n",
    "    incon_value = result_df['inc'].sum()\n",
    "    incon_value/len(D)\n",
    "\n",
    "    return incon_value\n",
    "\n",
    "def lvf(MAX_TRIES, D, TARGET_COLUMN, gamma):\n",
    "    C_best = len(D)\n",
    "    S_best = D.columns.values\n",
    "    M_sets=[]\n",
    "    \n",
    "    for i in range(1,MAX_TRIES):\n",
    "        S = sorted(randomSet(D.columns.values,C_best))\n",
    "        C = len(S)\n",
    "        if (C<C_best):\n",
    "            if(InconCheck(S,D,TARGET_COLUMN)<gamma):\n",
    "                M_sets=[]\n",
    "                S_best = S\n",
    "                C_best = C\n",
    "                M_sets.append(S_best)\n",
    "                print(\"\")\n",
    "                print(\"current best\", S_best)\n",
    "        elif ((C==C_best) and (InconCheck(S,D,TARGET_COLUMN)<gamma)) :\n",
    "            M_sets.append(S)\n",
    "            print(\"also current best\", S)\n",
    "    return M_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # put NOTE into paper - not sure if correct bayes\n",
    "from sklearn.model_selection import StratifiedKFold # put NOTE into paper - better as common kfold sampling\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from functools import reduce\n",
    "\n",
    "CI_95_FACTOR = 1.96\n",
    "\n",
    "class BaseClassifier:\n",
    "    \n",
    "    def __init__(self, clf, clf_name, modality):\n",
    "        self.clf = clf\n",
    "        self.clf_name = clf_name\n",
    "        self.modality = modality\n",
    "        \n",
    "    def fit(self, df_features, df_targets, verbose=True):\n",
    "        self.clf = clone(self.clf) # reset any previously trained model\n",
    "        np.random.seed(32143421)\n",
    "        if verbose:\n",
    "            print(f\"Starting training for classifier {self.clf_name} and modality {self.modality}\")\n",
    "        self.clf.fit(df_features, df_targets)\n",
    "        \n",
    "    def predict(self, df_features, verbose=True):\n",
    "        if verbose:\n",
    "            print(f\"Starting prediction for classifier {self.clf_name} and modality {self.modality}\")\n",
    "        return self.clf.predict(df_features)    \n",
    "        \n",
    "class ClassifierFactory:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_metadata_classifiers():\n",
    "        return [BaseClassifier(KNeighborsClassifier(), 'k-Nearest neighbors', 'Metadata'),\n",
    "                BaseClassifier(NearestCentroid(), 'Nearest mean classifier', 'Metadata'),\n",
    "                BaseClassifier(DecisionTreeClassifier(), 'Decision tree', 'Metadata'),\n",
    "                BaseClassifier(LogisticRegression(), 'Logistic regression', 'Metadata'),\n",
    "                BaseClassifier(SVC(gamma='auto'), 'SVM (Gaussian Kernel)', 'Metadata'),\n",
    "                BaseClassifier(BaggingClassifier(),'Bagging', 'Metadata'),\n",
    "                BaseClassifier(RandomForestClassifier(n_estimators=10),'Random Forest', 'Metadata'),\n",
    "                BaseClassifier(AdaBoostClassifier(), 'AdaBoost', 'Metadata'),\n",
    "                BaseClassifier(GradientBoostingClassifier(), 'Gradient Boosting Tree', 'Metadata')]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_text_classifiers():\n",
    "        return [BaseClassifier(GaussianNB(), 'Naive Bayes', 'Textual'),\n",
    "                BaseClassifier(KNeighborsClassifier(), 'k-Nearest neighbors', 'Textual'),\n",
    "                BaseClassifier(SVC(gamma='auto'), 'SVM (Gaussian Kernel)', 'Textual')]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_visual_classifiers():\n",
    "        return [BaseClassifier(KNeighborsClassifier(), 'k-Nearest neighbors', 'Visual'),\n",
    "                BaseClassifier(DecisionTreeClassifier(), 'Decision tree', 'Visual'),\n",
    "                BaseClassifier(LogisticRegression(), 'Logistic regression', 'Visual'),\n",
    "                BaseClassifier(SVC(gamma='auto'), 'SVM (Gaussian Kernel)', 'Visual'),\n",
    "                BaseClassifier(RandomForestClassifier(n_estimators=10), 'Random Forest', 'Visual'),\n",
    "                BaseClassifier(AdaBoostClassifier(), 'AdaBoost', 'Visual'),\n",
    "                BaseClassifier(GradientBoostingClassifier(), 'Gradient Boosting Tree', 'Visual')]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_audio_classifiers():\n",
    "        return [BaseClassifier(LogisticRegression(), 'Logistic regression', 'Audio'),\n",
    "                BaseClassifier(GradientBoostingClassifier(), 'Gradient Boosting Tree', 'Audio')]                \n",
    "                \n",
    "class DataWrapper:\n",
    "    \n",
    "    def __init__(self, df_audio, df_vis, df_txt, df_meta, df_targets):\n",
    "        self.df_audio = df_audio\n",
    "        self.df_vis = df_vis\n",
    "        self.df_txt = df_txt\n",
    "        self.df_meta = df_meta\n",
    "        self.df_targets = df_targets\n",
    "        \n",
    "    def generate_subspace(self):\n",
    "        # TODO insert LVW functionality\n",
    "        # N = len(df_meta)\n",
    "        # MAX_TRIES = 77*N^5\n",
    "        # print(lvf(MAX_TRIES,df_meta, df_targets_train,5))\n",
    "        \n",
    "        self.df_audio = self.df_audio.drop('file_name', axis=1)\n",
    "        self.df_vis = self.df_vis.drop('file_name', axis=1)\n",
    "        self.df_txt = self.df_txt.drop('file_name', axis=1)\n",
    "        self.df_meta = self.df_meta.drop('file_name', axis=1)\n",
    "        \n",
    "        \n",
    "class OutputWrapper:\n",
    "    \n",
    "    def __init__(self, df_res_test, df_score_test, df_score_stats, df_res_all):\n",
    "        self.df_res_test = df_res_test\n",
    "        self.df_score_test = df_score_test\n",
    "        self.df_score_stats = df_score_stats\n",
    "        self.df_res_all = df_res_all\n",
    "        \n",
    "class Evaluator:\n",
    "    # TODO add clf stacking method\n",
    "    \n",
    "    def __init__(self, data_wrapper, use_audio=True, use_visual=True, use_text=True, use_meta=True):\n",
    "        self.data_wrapper = data_wrapper\n",
    "        self.use_audio = use_audio\n",
    "        self.use_visual = use_visual\n",
    "        self.use_text = use_text\n",
    "        self.use_meta = use_meta            \n",
    "            \n",
    "    def cv_modality(self, df_features, df_targets, clfs, cv=10, verbose=True, predict_all = False):\n",
    "        kf = StratifiedKFold(n_splits=cv, random_state=9832432)\n",
    "        df_cvs = []\n",
    "        df_all_cvs = []\n",
    "        i=1\n",
    "        for train_index, test_index in kf.split(df_features, df_targets):\n",
    "            if verbose:\n",
    "                print(f\"Performing CV fold {i}..\")\n",
    "            i += 1\n",
    "            X_train, X_test = df_features.iloc[train_index,:], df_features.iloc[test_index,:]\n",
    "            y_train, y_test = df_targets[train_index], df_targets[test_index]\n",
    "            \n",
    "            df_res = pd.DataFrame(y_test)\n",
    "            df_res.columns = ['TARGET']\n",
    "            \n",
    "            # init df for prediction of all entries\n",
    "            df_res_all = pd.DataFrame(df_targets)\n",
    "            df_res_all.columns = ['TARGET']\n",
    "            \n",
    "            for clf in clfs:\n",
    "                clf.fit(X_train, y_train, verbose)\n",
    "                y_pred = clf.predict(X_test, verbose)\n",
    "                df_res[clf.clf_name+\"_\"+clf.modality] = y_pred\n",
    "                \n",
    "                if predict_all:\n",
    "                    y_pred_all = clf.predict(df_features, verbose)\n",
    "                    df_res_all[clf.clf_name+\"_\"+clf.modality] = y_pred_all\n",
    "                \n",
    "            df_cvs.append(df_res)\n",
    "            df_all_cvs.append(df_res_all)\n",
    "        return df_cvs, df_all_cvs\n",
    "            \n",
    "    def cv(self, cv=10, verbose=True, predict_all=False):\n",
    "        \"\"\"\n",
    "            set predict_all to True to also include predictions for all data\n",
    "        \"\"\"\n",
    "        df_cvs = []\n",
    "        df_all_cvs = []\n",
    "        df_targets = self.data_wrapper.df_targets\n",
    "        if (self.use_audio):\n",
    "            df_test, df_all = self.cv_modality(\n",
    "                self.data_wrapper.df_audio,\n",
    "                df_targets,\n",
    "                ClassifierFactory.get_audio_classifiers(),\n",
    "                cv,\n",
    "                verbose,\n",
    "                predict_all)\n",
    "            df_cvs.append(df_test)\n",
    "            df_all_cvs.append(df_all)\n",
    "        if (self.use_visual):\n",
    "            df_test, df_all = self.cv_modality(\n",
    "                self.data_wrapper.df_vis,\n",
    "                df_targets,\n",
    "                ClassifierFactory.get_visual_classifiers(),\n",
    "                cv,\n",
    "                verbose,\n",
    "                predict_all)\n",
    "            df_cvs.append(df_test)\n",
    "            df_all_cvs.append(df_all)\n",
    "        if (self.use_text):\n",
    "            df_test, df_all = self.cv_modality(\n",
    "                self.data_wrapper.df_txt,\n",
    "                df_targets,\n",
    "                ClassifierFactory.get_text_classifiers(),\n",
    "                cv,\n",
    "                verbose,\n",
    "                predict_all)\n",
    "            df_cvs.append(df_test)\n",
    "            df_all_cvs.append(df_all)\n",
    "        if (self.use_meta):\n",
    "            df_test, df_all = self.cv_modality(\n",
    "                self.data_wrapper.df_meta,\n",
    "                df_targets,\n",
    "                ClassifierFactory.get_metadata_classifiers(),\n",
    "                cv,\n",
    "                verbose,\n",
    "                predict_all)\n",
    "            df_cvs.append(df_test)\n",
    "            df_all_cvs.append(df_all)\n",
    "            \n",
    "        df_c = []\n",
    "        for i in range(len(df_cvs[0])):\n",
    "            df_i = pd.concat([df_cvs[x][i] for x in range(len(df_cvs))], axis=1)\n",
    "            df_i = df_i.loc[:,~df_i.columns.duplicated()]\n",
    "            df_c.append(df_i)\n",
    "            \n",
    "        df_all_c = []\n",
    "        if predict_all:\n",
    "            for i in range(len(df_all_cvs[0])):\n",
    "                df_i = pd.concat([df_all_cvs[x][i] for x in range(len(df_all_cvs))], axis=1)\n",
    "                df_i = df_i.loc[:,~df_i.columns.duplicated()]\n",
    "                df_all_c.append(df_i)\n",
    "            \n",
    "        df_res = self.evaluate(df_c)\n",
    "        \n",
    "        return OutputWrapper(df_c, df_res, self.overall_eval(df_res), df_all_c)\n",
    "\n",
    "    def evaluate(self, df_res):\n",
    "        \"\"\"\n",
    "        returns precision, recall and F1 in a DF\n",
    "        returns list of DFs if df_res is list of DFs\n",
    "        \"\"\"\n",
    "        if type(df_res) == type([]):\n",
    "            return [self.evaluate_single(df_x) for df_x in df_res]\n",
    "        else:\n",
    "            return self.evaluate_single(df_res)\n",
    "\n",
    "    def evaluate_single(self, df_res):\n",
    "        cols = list(df_res.columns)\n",
    "        cols.remove('TARGET')\n",
    "        df_ev = pd.DataFrame(columns=['clf', 'precision', 'recall', 'f1'])\n",
    "        for col in cols:\n",
    "            prec = precision_score(df_res['TARGET'], df_res[col])\n",
    "            recall = recall_score(df_res['TARGET'], df_res[col])\n",
    "            f1 = f1_score(df_res['TARGET'], df_res[col])\n",
    "            df_ev = df_ev.append({'clf':col, 'precision': prec, 'recall':recall, 'f1':f1}, ignore_index=True)\n",
    "        return df_ev\n",
    "    \n",
    "    def overall_eval(self, df_results):\n",
    "        df_score = pd.DataFrame(columns =['clf', 'mean_precision', 'mean_recall', 'mean_f1', 'var_precision', 'var_recall', 'var_f1', 'std_precision', 'std_recall', 'std_f1'])\n",
    "        df_score['clf'] = df_results[0]['clf']\n",
    "        # mean \n",
    "        df_res = reduce(lambda x, y: x.add(y, fill_value=0), df_results)\n",
    "        df_score[['mean_precision', 'mean_recall', 'mean_f1']] = df_res[['precision', 'recall', 'f1']].div(len(df_results))\n",
    "\n",
    "        # var\n",
    "        df_mean = df_score[['mean_precision', 'mean_recall', 'mean_f1']]\n",
    "        df_mean.columns = ['precision', 'recall', 'f1']\n",
    "        df_dev = []\n",
    "        for df in df_results:\n",
    "            df_dev.append(np.square(df[['precision', 'recall', 'f1']].subtract(df_mean)))\n",
    "        df_dev = reduce(lambda x, y: x.add(y, fill_value=0), df_dev)\n",
    "        df_score[['var_precision', 'var_recall', 'var_f1']] = df_dev[['precision', 'recall', 'f1']].div(len(df_results))\n",
    "\n",
    "        # std \n",
    "        df_score[['std_precision', 'std_recall', 'std_f1']] = np.power(df_score[['var_precision', 'var_recall', 'var_f1']], 0.5)\n",
    "        \n",
    "        # 95-ci\n",
    "        for metric in ['precision', 'recall', 'f1']:\n",
    "            l_ci = df_score['mean_'+metric] - df_score['std_'+metric]*CI_95_FACTOR/len(df_results)\n",
    "            u_ci = df_score['mean_'+metric] + df_score['std_'+metric]*CI_95_FACTOR/len(df_results)\n",
    "            df_score['l_95ci_'+metric] = l_ci\n",
    "            df_score['u_95ci_'+metric] = u_ci\n",
    "\n",
    "        return df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = DataWrapper(df_audio_train, df_vis_train, df_txt_train, df_meta_train, df_targets_train)\n",
    "dw.generate_subspace()\n",
    "ev = Evaluator(dw) # TODO preprocessing for meta needs to be done\n",
    "ow = ev.cv(verbose=False, predict_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ow.df_res_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ow.df_score_test[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ow.df_score_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ow.df_res_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
